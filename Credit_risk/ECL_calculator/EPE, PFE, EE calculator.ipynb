{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ad1d1-3e4c-4aeb-a1be-5cd096d5a448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6782de-68f3-4fd2-91d8-40f455a0aa94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed7c0b9-3a13-4a00-a4ec-dff96a28998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run timestamp: 2025-09-01T17:14:56.534418\n",
      "\n",
      "Test set size: 589\n",
      "AUC: 0.6837\n",
      "Optimal threshold (Youden's J): 0.138\n",
      "Confusion Matrix @ threshold:\n",
      "  TP=69, TN=248, FP=253, FN=19\n",
      "\n",
      "Files created:\n",
      "- .\\PD_EAD_ECL_Report.xlsx\n",
      "- .\\pd_distribution.png\n",
      "- .\\roc_curve.png\n",
      "- .\\calibration_plot.png\n",
      "- .\\exposure_over_time.png\n",
      "- .\\ecl_by_segment.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pd_ead_ecl_pipeline_legends.py\n",
    "# -----------------------------------------------------------\n",
    "# End-to-end PD (Logit) + Exposure (EE/PFE/EAD) + ECL pipeline\n",
    "# Generates synthetic data, trains a logistic model, evaluates,\n",
    "# estimates EAD from EE/PFE, computes ECL, plots results (with legends),\n",
    "# and writes a multi-sheet Excel report.\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0) CONFIG\n",
    "# -------------------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "OUTPUT_DIR = \".\"  # change to \"/mnt/data\" if you prefer\n",
    "PLOT_DPI = 150\n",
    "PFE_QUANTILE = 0.95     # quantile for PFE\n",
    "LAMBDA_EAD = 0.5        # EAD blend: EAD = EE + lambda * PFE\n",
    "BASE_VOL = 0.10         # base relative volatility for exposure\n",
    "ASSUMED_LGD = 0.45      # constant LGD (can be replaced with model)\n",
    "HORIZONS_MONTHS = np.array([1, 3, 6, 12])  # EAD horizons\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) SYNTHETIC DATA GENERATION\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def gen_synthetic_credit_data(n=2000):\n",
    "    \"\"\"\n",
    "    Create a synthetic dataset with intuitive relationships between features\n",
    "    and default probability using a latent logistic function.\n",
    "    \"\"\"\n",
    "    credit_score = np.clip(np.round(np.random.normal(680, 70, n)), 300, 850)\n",
    "    loan_amount = np.round(np.random.lognormal(mean=11.0, sigma=0.7, size=n))\n",
    "    loan_amount = np.clip(loan_amount, 5e4, 1e7)\n",
    "    tenure_months = np.random.randint(6, 121, size=n)\n",
    "    interest_rate = np.round(np.random.uniform(0.06, 0.24, size=n), 4)\n",
    "    utilization = np.round(np.random.uniform(0.3, 1.0, size=n), 3)\n",
    "    limit_amount = np.round(loan_amount / np.maximum(0.01, utilization))\n",
    "\n",
    "    # Latent PD via logistic with intuitive signs\n",
    "    x_score = (700 - credit_score) / 100.0\n",
    "    x_amt = np.log(loan_amount) - np.log(2e5)\n",
    "    x_ten = (tenure_months - 36) / 12.0\n",
    "    x_rate = (interest_rate - 0.12) * 10\n",
    "    x_util = (utilization - 0.7) * 2\n",
    "\n",
    "    linpred = -2.2 + 0.8*x_score + 0.25*x_amt + 0.15*x_ten + 0.4*x_rate + 0.5*x_util\n",
    "    true_pd = 1.0 / (1.0 + np.exp(-linpred))\n",
    "    default_flag = (np.random.uniform(size=n) < true_pd).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'counterparty_id': [f'C{i:05d}' for i in range(1, n+1)],\n",
    "        'credit_score': credit_score.astype(int),\n",
    "        'loan_amount': loan_amount.astype(float),\n",
    "        'limit_amount': limit_amount.astype(float),\n",
    "        'tenure_months': tenure_months.astype(int),\n",
    "        'interest_rate': interest_rate.astype(float),\n",
    "        'utilization': utilization.astype(float),\n",
    "        'default_flag': default_flag.astype(int),\n",
    "        'true_pd': true_pd\n",
    "    })\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) PD MODELING (LOGISTIC REGRESSION)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def prepare_features(df):\n",
    "    df = df.copy()\n",
    "    df['log_loan_amt'] = np.log(df['loan_amount'])\n",
    "    df['rate_pct'] = df['interest_rate'] * 100.0\n",
    "    df['score_centered'] = df['credit_score'] - 700\n",
    "    model_cols = ['score_centered', 'log_loan_amt', 'tenure_months', 'rate_pct', 'utilization']\n",
    "    X = sm.add_constant(df[model_cols])\n",
    "    y = df['default_flag'].astype(int)\n",
    "    return X, y, model_cols\n",
    "\n",
    "def fit_logit(X, y):\n",
    "    return sm.Logit(y, X).fit(disp=False)\n",
    "\n",
    "def roc_curve_manual(y_true, y_score, num=400):\n",
    "    thresholds = np.linspace(0, 1, num)\n",
    "    tpr_list, fpr_list = [], []\n",
    "    P = (y_true == 1).sum()\n",
    "    N = (y_true == 0).sum()\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_score >= t).astype(int)\n",
    "        TP = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "        FP = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "        TPR = TP / P if P > 0 else 0.0\n",
    "        FPR = FP / N if N > 0 else 0.0\n",
    "        tpr_list.append(TPR)\n",
    "        fpr_list.append(FPR)\n",
    "    return np.array(fpr_list), np.array(tpr_list), thresholds\n",
    "\n",
    "def auc_trapz(fpr, tpr):\n",
    "    sort_idx = np.argsort(fpr)\n",
    "    return np.trapz(tpr[sort_idx], fpr[sort_idx]), sort_idx\n",
    "\n",
    "def confusion_at_threshold(y_true, y_score, threshold):\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "    TP = int(((y_pred == 1) & (y_true == 1)).sum())\n",
    "    TN = int(((y_pred == 0) & (y_true == 0)).sum())\n",
    "    FP = int(((y_pred == 1) & (y_true == 0)).sum())\n",
    "    FN = int(((y_pred == 0) & (y_true == 1)).sum())\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def calibration_curve_df(y_true, y_prob, n_bins=10):\n",
    "    df = pd.DataFrame({'y': y_true, 'p': y_prob}).copy()\n",
    "    df['bin'] = pd.qcut(df['p'], q=n_bins, duplicates='drop')\n",
    "    g = df.groupby('bin', observed=True).agg(\n",
    "        avg_p=('p', 'mean'),\n",
    "        obs_rate=('y', 'mean'),\n",
    "        count=('y', 'size')\n",
    "    ).reset_index(drop=True)\n",
    "    return g\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) EXPOSURE CALCULATION (EE/PFE) & EAD\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def balance_at_t(loan_amt, tenure_m, t_month):\n",
    "    \"\"\"Linear amortization of principal to horizon t.\"\"\"\n",
    "    frac = np.clip(1.0 - t_month / np.maximum(tenure_m, 1), 0.0, 1.0)\n",
    "    return loan_amt * frac\n",
    "\n",
    "def ee_trunc_normal(mu, sigma):\n",
    "    \"\"\"\n",
    "    EE for X = max(0, N(mu, sigma^2))\n",
    "    EE = sigma*phi(a) + mu*Phi(a), a = mu/sigma\n",
    "    \"\"\"\n",
    "    if sigma <= 0:\n",
    "        return max(mu, 0.0)\n",
    "    a = mu / sigma\n",
    "    return sigma * norm.pdf(a) + mu * norm.cdf(a)\n",
    "\n",
    "def pfe_trunc_normal(mu, sigma, quantile=PFE_QUANTILE):\n",
    "    \"\"\"PFE_q = max(0, mu + sigma*z_q) under normal approximation.\"\"\"\n",
    "    z = norm.ppf(quantile)\n",
    "    return max(mu + sigma * z, 0.0)\n",
    "\n",
    "def compute_counterparty_ead_rows(test_df, horizons=HORIZONS_MONTHS,\n",
    "                                  pfe_q=PFE_QUANTILE, lambda_ead=LAMBDA_EAD,\n",
    "                                  base_vol=BASE_VOL, assumed_lgd=ASSUMED_LGD):\n",
    "    rows = []\n",
    "    median_rate = test_df['interest_rate'].median()\n",
    "\n",
    "    for _, r in test_df.iterrows():\n",
    "        limit_amt = float(r['limit_amount'])\n",
    "        vol = base_vol * (1 + (r['interest_rate'] - median_rate) / 0.12) * (0.8 + 0.4*r['utilization'])\n",
    "        vol = float(np.clip(vol, 0.02, 0.50))\n",
    "\n",
    "        eads_t = []\n",
    "        for t in horizons:\n",
    "            mu = balance_at_t(r['loan_amount'], r['tenure_months'], t)\n",
    "            sigma = vol * mu * np.sqrt(t / 12.0)\n",
    "            EE_t = ee_trunc_normal(mu, sigma)\n",
    "            PFE_t = pfe_trunc_normal(mu, sigma, pfe_q)\n",
    "            EAD_t = min(limit_amt, EE_t + lambda_ead * PFE_t)\n",
    "            eads_t.append(EAD_t)\n",
    "\n",
    "        EAD_i = float(np.max(eads_t))  # conservative\n",
    "        ECL_i = float(r['pd_hat'] * EAD_i * assumed_lgd)\n",
    "        rows.append({\n",
    "            'counterparty_id': r['counterparty_id'],\n",
    "            'PD_hat': float(r['pd_hat']),\n",
    "            'LGD_assumed': assumed_lgd,\n",
    "            'EAD': EAD_i,\n",
    "            'ECL': ECL_i,\n",
    "            'credit_score': int(r['credit_score']),\n",
    "            'loan_amount': float(r['loan_amount']),\n",
    "            'tenure_months': int(r['tenure_months']),\n",
    "            'interest_rate': float(r['interest_rate']),\n",
    "            'utilization': float(r['utilization'])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def aggregate_exposure_over_time(test_df, horizons=HORIZONS_MONTHS,\n",
    "                                 pfe_q=PFE_QUANTILE, base_vol=BASE_VOL):\n",
    "    agg_rows = []\n",
    "    median_rate = test_df['interest_rate'].median()\n",
    "    for t in horizons:\n",
    "        ee_sum = 0.0\n",
    "        pfe_sum = 0.0\n",
    "        for _, r in test_df.iterrows():\n",
    "            vol = base_vol * (1 + (r['interest_rate'] - median_rate) / 0.12) * (0.8 + 0.4*r['utilization'])\n",
    "            vol = float(np.clip(vol, 0.02, 0.50))\n",
    "            mu = balance_at_t(r['loan_amount'], r['tenure_months'], t)\n",
    "            sigma = vol * mu * np.sqrt(t / 12.0)\n",
    "            EE_t = ee_trunc_normal(mu, sigma)\n",
    "            PFE_t = pfe_trunc_normal(mu, sigma, pfe_q)\n",
    "            ee_sum += EE_t\n",
    "            pfe_sum += PFE_t     # conservative aggregation (sum quantiles)\n",
    "        agg_rows.append({'t_month': int(t), 'EE_sum': ee_sum, 'PFE_sum_conservative': pfe_sum})\n",
    "    return pd.DataFrame(agg_rows)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) VISUALIZATIONS (with legends)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def plot_pd_distribution_with_legend(risk_df, path):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(\n",
    "        risk_df['PD_hat'],\n",
    "        bins=30, stat='count', color='#1f77b4', alpha=0.6,\n",
    "        ax=ax, label='Histogram'\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        risk_df['PD_hat'],\n",
    "        color='#ff7f0e', lw=2, ax=ax, label='KDE'\n",
    "    )\n",
    "    ax.set_title('Predicted PD Distribution (Test Set)')\n",
    "    ax.set_xlabel('PD')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend(title='Components', frameon=True)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=PLOT_DPI)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_roc_with_legend(fpr, tpr, sort_idx, auc, path):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.plot(\n",
    "        fpr[sort_idx], tpr[sort_idx],\n",
    "        label=f'Model ROC (AUC={auc:.3f})', color='#2ca02c', lw=2\n",
    "    )\n",
    "    ax.plot([0, 1], [0, 1], '--', color='gray', lw=1.2, label='Random chance')\n",
    "    ax.set_title('ROC Curve (Test Set)')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.legend(loc='lower right', frameon=True)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=PLOT_DPI)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_calibration_with_legend(calib_df, path):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.plot([0, 1], [0, 1], '--', color='gray', lw=1.2, label='Perfect calibration')\n",
    "    ax.plot(\n",
    "        calib_df['avg_p'], calib_df['obs_rate'],\n",
    "        marker='o', color='#ff7f0e', label='Binned observed rate'\n",
    "    )\n",
    "    ax.set_title('Calibration Plot (Test Set)')\n",
    "    ax.set_xlabel('Average Predicted PD (bin)')\n",
    "    ax.set_ylabel('Observed Default Rate (bin)')\n",
    "    ax.legend(frameon=True)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    # Optional: annotate bin sizes\n",
    "    # for x, y, n in zip(calib_df['avg_p'], calib_df['obs_rate'], calib_df['count']):\n",
    "    #     ax.annotate(f'n={n}', (x, y), textcoords='offset points', xytext=(6, 6), fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=PLOT_DPI)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_exposure_over_time_with_legend(agg_expo_df, path, pfe_quantile=PFE_QUANTILE):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(\n",
    "        agg_expo_df['t_month'], agg_expo_df['EE_sum'] / 1e6,\n",
    "        marker='o', label='EE (sum)', color='#1f77b4'\n",
    "    )\n",
    "    ax.plot(\n",
    "        agg_expo_df['t_month'], agg_expo_df['PFE_sum_conservative'] / 1e6,\n",
    "        marker='s', label=f'PFE {int(pfe_quantile*100)}% (sum, conservative)', color='#d62728'\n",
    "    )\n",
    "    ax.set_title('Portfolio Exposure over Time')\n",
    "    ax.set_xlabel('Months Ahead')\n",
    "    ax.set_ylabel('Exposure (INR millions)')  # INR to avoid font warnings\n",
    "    ax.legend(frameon=True)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=PLOT_DPI)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_ecl_by_segment_with_legend(segment_ecl_df, path, lgd=ASSUMED_LGD):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.barplot(\n",
    "        x='score_band', y='ECL', data=segment_ecl_df,\n",
    "        ax=ax, color='#9467bd', label=f'ECL (LGD={int(lgd*100)}%)'\n",
    "    )\n",
    "    ax.set_title('ECL by Credit Score Segment')\n",
    "    ax.set_xlabel('Score Band')\n",
    "    ax.set_ylabel('ECL (INR)')\n",
    "    ax.legend(frameon=True)\n",
    "    ax.ticklabel_format(axis='y', style='plain', useOffset=False)\n",
    "    ax.grid(axis='y', alpha=0.2)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=PLOT_DPI)\n",
    "    plt.close(fig)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) REPORTING (EXCEL)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def write_excel_report(out_path, params_df, summary_df, coef_table, risk_sorted, agg_expo, segment_ecl):\n",
    "    with pd.ExcelWriter(out_path, engine='openpyxl') as writer:\n",
    "        params_df.to_excel(writer, sheet_name='Inputs', index=False)\n",
    "        summary_df.to_excel(writer, sheet_name='PD_Model_Summary', index=False)\n",
    "        coef_table.to_excel(writer, sheet_name='Logit_Coefficients', index=False)\n",
    "        risk_sorted.to_excel(writer, sheet_name='Counterparty_Risk', index=False)\n",
    "        agg_expo.to_excel(writer, sheet_name='EE_PFE_by_T', index=False)\n",
    "        segment_ecl.to_excel(writer, sheet_name='Segment_ECL', index=False)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# MAIN\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # 1) Data\n",
    "    raw = gen_synthetic_credit_data(n=2000)\n",
    "    msk = np.random.rand(len(raw)) < 0.7\n",
    "    train = raw[msk].copy()\n",
    "    test = raw[~msk].copy()\n",
    "\n",
    "    # 2) PD Modeling\n",
    "    X_train, y_train, model_cols = prepare_features(train)\n",
    "    result = fit_logit(X_train, y_train)\n",
    "\n",
    "    X_test, y_test, _ = prepare_features(test)\n",
    "    proba_test = result.predict(X_test)\n",
    "    test = test.assign(pd_hat=proba_test)\n",
    "\n",
    "    # Metrics\n",
    "    fpr, tpr, thr = roc_curve_manual(y_test.values, test['pd_hat'].values, num=400)\n",
    "    auc, sort_idx = auc_trapz(fpr, tpr)\n",
    "    j_scores = tpr - fpr\n",
    "    best_idx = int(np.argmax(j_scores))\n",
    "    best_thr = float(thr[best_idx])\n",
    "    TP, TN, FP, FN = confusion_at_threshold(y_test.values, test['pd_hat'].values, best_thr)\n",
    "    calib = calibration_curve_df(y_test, test['pd_hat'], n_bins=10)\n",
    "\n",
    "    # 3) Exposure and EAD (per counterparty + aggregate)\n",
    "    risk_df = compute_counterparty_ead_rows(\n",
    "        test_df=test,\n",
    "        horizons=HORIZONS_MONTHS,\n",
    "        pfe_q=PFE_QUANTILE,\n",
    "        lambda_ead=LAMBDA_EAD,\n",
    "        base_vol=BASE_VOL,\n",
    "        assumed_lgd=ASSUMED_LGD\n",
    "    )\n",
    "    agg_expo = aggregate_exposure_over_time(\n",
    "        test_df=test,\n",
    "        horizons=HORIZONS_MONTHS,\n",
    "        pfe_q=PFE_QUANTILE,\n",
    "        base_vol=BASE_VOL\n",
    "    )\n",
    "\n",
    "    # Score segments for reporting\n",
    "    bins = [0, 580, 670, 740, 800, 1000]\n",
    "    labels = ['Poor (<580)', 'Fair (580-669)', 'Good (670-739)', 'Very Good (740-799)', 'Excellent (800+)']\n",
    "    risk_df['score_band'] = pd.cut(risk_df['credit_score'], bins=bins, labels=labels, right=False)\n",
    "    segment_ecl = risk_df.groupby('score_band', observed=True)['ECL'].sum().reset_index()\n",
    "\n",
    "    # 4) Visualizations with legends\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    pd_dist_path = os.path.join(OUTPUT_DIR, 'pd_distribution.png')\n",
    "    roc_path = os.path.join(OUTPUT_DIR, 'roc_curve.png')\n",
    "    calib_path = os.path.join(OUTPUT_DIR, 'calibration_plot.png')\n",
    "    expo_path = os.path.join(OUTPUT_DIR, 'exposure_over_time.png')\n",
    "    ecl_seg_path = os.path.join(OUTPUT_DIR, 'ecl_by_segment.png')\n",
    "\n",
    "    plot_pd_distribution_with_legend(risk_df, pd_dist_path)\n",
    "    plt.show()  # Display the PD distribution plot\n",
    "    plot_roc_with_legend(fpr, tpr, sort_idx, auc, roc_path)\n",
    "    plt.show()  # Display the ROC curve plot\n",
    "    plot_calibration_with_legend(calib, calib_path)\n",
    "    plt.show()  # Display the calibration plot\n",
    "    plot_exposure_over_time_with_legend(agg_expo, expo_path, pfe_quantile=PFE_QUANTILE)\n",
    "    plt.show()  # Display the exposure over time plot\n",
    "    plot_ecl_by_segment_with_legend(segment_ecl, ecl_seg_path, lgd=ASSUMED_LGD)\n",
    "    plt.show()  # Display the ECL by segment plot\n",
    "\n",
    "\n",
    "\n",
    "    # 5) Reporting\n",
    "    summary = pd.DataFrame({\n",
    "        'metric': ['AUC', 'Best_Threshold', 'TP', 'TN', 'FP', 'FN'],\n",
    "        'value': [auc, best_thr, TP, TN, FP, FN]\n",
    "    })\n",
    "    coef_table = result.summary2().tables[1].reset_index()\n",
    "    coef_table.rename(columns={'index': 'variable'}, inplace=True)\n",
    "    params_df = pd.DataFrame({\n",
    "        'parameter': ['LGD', 'PFE_quantile', 'lambda_EAD', 'base_volatility'],\n",
    "        'value': [ASSUMED_LGD, PFE_QUANTILE, LAMBDA_EAD, BASE_VOL]\n",
    "    })\n",
    "\n",
    "    risk_sorted = risk_df.sort_values('ECL', ascending=False)\n",
    "    out_xlsx = os.path.join(OUTPUT_DIR, 'PD_EAD_ECL_Report.xlsx')\n",
    "    write_excel_report(out_xlsx, params_df, summary, coef_table, risk_sorted, agg_expo, segment_ecl)\n",
    "\n",
    "    text_summary = f\"\"\"\n",
    "Run timestamp: {datetime.now().isoformat()}\n",
    "\n",
    "Test set size: {len(test)}\n",
    "AUC: {auc:.4f}\n",
    "Optimal threshold (Youden's J): {best_thr:.3f}\n",
    "Confusion Matrix @ threshold:\n",
    "  TP={TP}, TN={TN}, FP={FP}, FN={FN}\n",
    "\n",
    "Files created:\n",
    "- {out_xlsx}\n",
    "- {pd_dist_path}\n",
    "- {roc_path}\n",
    "- {calib_path}\n",
    "- {expo_path}\n",
    "- {ecl_seg_path}\n",
    "\"\"\"\n",
    "    with open(os.path.join(OUTPUT_DIR, 'run_summary.txt'), 'w') as f:\n",
    "        f.write(text_summary.strip() + \"\\n\")\n",
    "    print(text_summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa937820-dc50-45ca-970f-9e529b72a328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
